{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number translation using RNN's (Encoder-Decoder with attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "from utils import tokenize\n",
    "from utils import build_vocabulary_token\n",
    "from utils import vectorize_corpus\n",
    "from utils import to_sentence\n",
    "import csv\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_train=np.load('data_npy/fr_train.npy')\n",
    "num_train=np.load('data_npy/num_train.npy')\n",
    "rev_shared_vocab=np.load('data_npy/rev_shared_vocab.npy')\n",
    "fr_val=np.load('data_npy/fr_val.npy')\n",
    "num_val=np.load('data_npy/num_val.npy')\n",
    "fr_test=np.load('data_npy/fr_test.npy')\n",
    "num_test=np.load('data_npy/num_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the shared dictionary\n",
    "tokenized_fr_train = [tokenize(s, word_level=True) for s in fr_train]\n",
    "tokenized_num_train = [tokenize(s, word_level=False) for s in num_train]\n",
    "shared_vocab, rev_shared_vocab = build_vocabulary_token(tokenized_fr_train+tokenized_num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons le dictionnaire bilingue utilisé lors du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_PAD',\n",
       " '_GO',\n",
       " '_EOS',\n",
       " '_UNK',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'cent',\n",
       " 'cents',\n",
       " 'cinq',\n",
       " 'cinquante',\n",
       " 'deux',\n",
       " 'dix',\n",
       " 'douze',\n",
       " 'et',\n",
       " 'huit',\n",
       " 'mille',\n",
       " 'neuf',\n",
       " 'onze',\n",
       " 'quarante',\n",
       " 'quatorze',\n",
       " 'quatre',\n",
       " 'quinze',\n",
       " 'seize',\n",
       " 'sept',\n",
       " 'six',\n",
       " 'soixante',\n",
       " 'treize',\n",
       " 'trente',\n",
       " 'trois',\n",
       " 'un',\n",
       " 'vingt',\n",
       " 'vingts']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_shared_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training, evaluating and testing sets -> vectorize_corpus mainly applies tokenization\n",
    "X_train, Y_train = vectorize_corpus(fr_train, num_train, shared_vocab,word_level_target=False)\n",
    "X_val, Y_val = vectorize_corpus(fr_val, num_val, shared_vocab,word_level_target=False)\n",
    "X_test, Y_test = vectorize_corpus(fr_test, num_test, shared_vocab,word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(torch.tensor(X_train[i], dtype=torch.long, device=device).view(-1, 1)\n",
    ",torch.tensor(Y_train[i], dtype=torch.long, device=device).view(-1, 1)) for i in range(num_train.shape[0])]\n",
    "\n",
    "eval_pairs=[(torch.tensor(X_val[i], dtype=torch.long, device=device).view(-1, 1)\n",
    ",torch.tensor(Y_val[i], dtype=torch.long, device=device).view(-1, 1)) for i in range(num_val.shape[0])]\n",
    "\n",
    "test_pairs=[(torch.tensor(X_test[i], dtype=torch.long, device=device).view(-1, 1)\n",
    ",torch.tensor(Y_test[i], dtype=torch.long, device=device).view(-1, 1)) for i in range(num_test.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple de pair d'entrainement utilisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[18],\n",
       "         [14],\n",
       "         [28],\n",
       "         [23],\n",
       "         [24],\n",
       "         [14],\n",
       "         [38],\n",
       "         [18],\n",
       "         [ 2],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0]]), tensor([[ 6],\n",
       "         [ 4],\n",
       "         [ 8],\n",
       "         [13],\n",
       "         [ 6],\n",
       "         [ 6],\n",
       "         [ 2],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0][0], pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the parameters we use\n",
    "MAX_LENGTH = 20\n",
    "vocab_size = len(shared_vocab)\n",
    "batch_size=32\n",
    "\n",
    "GO_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "config ={\n",
    "        'dropout': 0.2,\n",
    "        'vocab_size': vocab_size,\n",
    "        'num_layers': 1,\n",
    "        'embsize': 32,\n",
    "        'dim_recurrent': 256,\n",
    "        'batch_size':batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'Encoder\n",
    "\n",
    "L'encoder d'un réseau seq2seq est un RNN qui renvoie une valeur pour chaque mot de l'input. Pour chaque mot de l'input, l'encoder renvoie un vecteur et un hidden state, qu'il utilise ensuite pour le mot suivant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.param = [input_size,hidden_size]\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)#(self.input_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #input2 = torch.tensor(input)\n",
    "        #print('avant call', input.dtype)\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Decoder\n",
    "\n",
    "Le decoder est un autre RNN qui prend les vecteurs de sortie de l'encoder et renvoie une liste de chiffres qui correspond à la traduction finale.\n",
    "\n",
    "### Attention\n",
    "\n",
    "L'attention permet au decoder, à chaque étape où le decoder renvoie ses outputs, de se concentrer sur une partie différente de la sortie de l'encoder. D'abord on calcule une liste de poids d'attention. On multiplie les vecteurs de sorties de l'encoder par ce vecteur de poids. Le resultat est un vecteur qui contient seulement l'information interessante sur laquelle on souhaite que le decoder se concentre.\n",
    "\n",
    "Pour calculer les poids d'attention, on utilise un couche de neurone attn, qui utilise comme input l'input du decoder et le hidden state.\n",
    "On choisit un vector de poids de taille 20, la taille maximale de nos inputs et outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        #Generate attention weigths \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        #Multiply the vector of weigths with the encoder outputs\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions\n",
    "\n",
    "Pour le training, on passe l'input dans l'encoder et on memorise chaque sortie et le dernier hidden state.\n",
    "Ensuite on donne au decoder le token <GO> comme 1er input du decoder et le dernier hidden state comme 1er hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "#Teaching forcing : on utilise a chaque la sortie reelle (vraie traduction) à chaque fois que le decodeur change de mot, au lieu d'utiliser l'estimation du decodeur sur le mot précédent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[GO_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluer la qualité de la prédiction\n",
    "def evaluate(test_pair, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    input_tensor=test_pair[0]\n",
    "    target_tensor=test_pair[1]\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "        loss2 = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[GO_token]])\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            loss2 += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return loss2.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On appelle la fonction train plusieurs fois et on affiche le progrès\n",
    "def trainIters(encoder, decoder, n_iters,epochs=5, print_every=100, plot_every=100, learning_rate=0.01,n_evaluate=1000):\n",
    " \n",
    "    all_losses=[]\n",
    "    all_test_losses=[]\n",
    "\n",
    "    start = time.time()\n",
    "    print_loss_total = 0.  # Reset every print_every\n",
    "    plot_loss_total = 0. # Reset every plot_every\n",
    "\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [random.choice(pairs)for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(0,epochs):\n",
    "\n",
    "        for iter in range(1, n_iters + 1):\n",
    "            training_pair = training_pairs[iter - 1]\n",
    "            input_tensor = torch.tensor(training_pair[0])\n",
    "            target_tensor = torch.tensor(training_pair[1])\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print(\"epoch :\" + str(epoch) + \" iter : \" + str(iter))\n",
    "                print( print_loss_avg)\n",
    "\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_loss_total = 0.\n",
    "                all_losses+=[plot_loss_avg]\n",
    "\n",
    "                test_loss=0.\n",
    "                for i in range(n_evaluate):\n",
    "                    test_loss+=evaluate(eval_pair=eval_pairs[i],encoder=encoder, decoder=decoder, criterion=criterion, max_length=MAX_LENGTH)\n",
    "                all_test_losses+=[test_loss/n_evaluate]\n",
    "\n",
    "\n",
    "        loss_PATH=\"losses_epoch\"+str(epoch)\n",
    "        loss_test_PATH=\"losses_test_epoch\"+str(epoch)\n",
    "        np.save(loss_PATH,np.array(all_losses))\n",
    "        np.save(loss_test_PATH,np.array(all_test_losses))\n",
    "\n",
    "        enco_PATH=\"encoder_epoch\"+str(epoch)\n",
    "        deco_PATH=\"decoder_epoch\"+str(epoch)\n",
    "        torch.save(encoder,enco_PATH)\n",
    "        torch.save(encoder,deco_PATH)\n",
    "\n",
    "    torch.save(encoder,\"second_enco\")\n",
    "    torch.save(encoder,\"second_deco\")       \n",
    "    np.save('all_losses',np.array(all_losses))\n",
    "    np.save('all_test_losses',np.array(all_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/anaconda3/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0 iter : 100\n",
      "0.8553461551666257\n",
      "epoch :0 iter : 200\n",
      "0.6740361294746396\n",
      "epoch :0 iter : 300\n",
      "0.689356122493744\n",
      "epoch :0 iter : 400\n",
      "0.6777678551673888\n",
      "epoch :0 iter : 500\n",
      "0.6824280066490176\n",
      "epoch :0 iter : 600\n",
      "0.6665150694847106\n",
      "epoch :0 iter : 700\n",
      "0.6553200592994692\n",
      "epoch :0 iter : 800\n",
      "0.6632100200653076\n",
      "epoch :0 iter : 900\n",
      "0.6459337892532347\n",
      "epoch :0 iter : 1000\n",
      "0.6473236408233644\n",
      "epoch :0 iter : 1100\n",
      "0.655437062740326\n",
      "epoch :0 iter : 1200\n",
      "0.6296192998886108\n",
      "epoch :0 iter : 1300\n",
      "0.6371200051307679\n",
      "epoch :0 iter : 1400\n",
      "0.6386592783927919\n",
      "epoch :0 iter : 1500\n",
      "0.6393913276195525\n",
      "epoch :0 iter : 1600\n",
      "0.6390984573364258\n",
      "epoch :0 iter : 1700\n",
      "0.6271481351852419\n",
      "epoch :0 iter : 1800\n",
      "0.6050196166038512\n",
      "epoch :0 iter : 1900\n",
      "0.5333645849227908\n",
      "epoch :0 iter : 2000\n",
      "0.5013874466419221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7f812863d9ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f38703e7d88e>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, epochs, print_every, plot_every, learning_rate, n_evaluate)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mtest_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mall_test_losses\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_evaluate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-21cf86835530>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(test_pair, encoder, decoder, criterion, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3ef9a7945584>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(config['vocab_size'], hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, config['vocab_size'], dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, n_iters=20000,epochs=10, print_every=100,plot_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n",
    "Here are the losses we got when we trained our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loss1 = np.load('result/losses.npy')\n",
    "loss2 =np.load('result/losses_test.npy')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss1, label='training_loss')\n",
    "plt.plot(loss2, label='evaluating_loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "After training, we want to test whether our model is able to provide correct translation. We test it with de testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a function decode that take a tokenized sentence of words and translates it into figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode la solution pour chaque input\n",
    "def decode(input_tensor,encoder,decoder):\n",
    "\n",
    "    max_length=MAX_LENGTH\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = MAX_LENGTH\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[GO_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    solu=torch.zeros(max_length)\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        solu[di]=decoder_input\n",
    "\n",
    "\n",
    "    return solu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MathieuRita/Desktop/TensorFlow/lib/python2.7/site-packages/torch/serialization.py:400: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/Users/MathieuRita/Desktop/TensorFlow/lib/python2.7/site-packages/torch/serialization.py:400: UserWarning: Couldn't retrieve source code for container of type AttnDecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "our_encoder=torch.load('saved_models/first_enco.pt')\n",
    "our_decoder=torch.load('saved_models/first_deco.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0\n",
      "input : quatre vingt dix neuf mille cinq cent dix huit\n",
      "correct answer :  99518\n",
      "prediction : 99518\n",
      "Test 1\n",
      "input : cinq cent cinquante cinq mille cinq cent quarante six\n",
      "correct answer :  555546\n",
      "prediction : 555546\n",
      "Test 2\n",
      "input : trois cent vingt deux mille cinq cent quatre vingt huit\n",
      "correct answer :  322588\n",
      "prediction : 322588\n",
      "Test 3\n",
      "input : mille neuf cent vingt et un\n",
      "correct answer :  1921\n",
      "prediction : 1921\n",
      "Test 4\n",
      "input : trois cent soixante douze mille dix sept\n",
      "correct answer :  372017\n",
      "prediction : 372017\n",
      "Test 5\n",
      "input : cinquante neuf mille cent soixante deux\n",
      "correct answer :  59162\n",
      "prediction : 59162\n",
      "Test 6\n",
      "input : vingt mille quatre cent quatre\n",
      "correct answer :  20404\n",
      "prediction : 20404\n",
      "Test 7\n",
      "input : vingt neuf mille deux\n",
      "correct answer :  29002\n",
      "prediction : 29002\n",
      "Test 8\n",
      "input : cinq cent trente huit mille cent soixante sept\n",
      "correct answer :  538167\n",
      "prediction : 538167\n",
      "Test 9\n",
      "input : six cent dix sept mille six cent quatre vingt trois\n",
      "correct answer :  617683\n",
      "prediction : 617683\n",
      "Test 10\n",
      "input : dix sept mille trois cent quatre vingt onze\n",
      "correct answer :  17391\n",
      "prediction : 17391\n",
      "Test 11\n",
      "input : cinquante quatre mille quatre cent douze\n",
      "correct answer :  54412\n",
      "prediction : 54412\n",
      "Test 12\n",
      "input : dix neuf mille neuf cent vingt deux\n",
      "correct answer :  19922\n",
      "prediction : 19922\n",
      "Test 13\n",
      "input : onze mille deux cent quarante trois\n",
      "correct answer :  11243\n",
      "prediction : 11243\n",
      "Test 14\n",
      "input : huit cent dix sept mille vingt cinq\n",
      "correct answer :  817025\n",
      "prediction : 817025\n",
      "Test 15\n",
      "input : cinquante mille cent cinquante cinq\n",
      "correct answer :  50155\n",
      "prediction : 50155\n",
      "Test 16\n",
      "input : cent trente neuf mille trente sept\n",
      "correct answer :  139037\n",
      "prediction : 139037\n",
      "Test 17\n",
      "input : sept cent dix mille deux cent cinquante quatre\n",
      "correct answer :  710254\n",
      "prediction : 710254\n",
      "Test 18\n",
      "input : trois mille huit cent quatre vingt quatre\n",
      "correct answer :  3884\n",
      "prediction : 3884\n",
      "Test 19\n",
      "input : quarante neuf mille neuf cent cinquante cinq\n",
      "correct answer :  49955\n",
      "prediction : 49955\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(\"Test \" + str(i))\n",
    "    randind=np.random.randint(0,len(fr_test))\n",
    "    print(\"input :\", fr_test[randind])\n",
    "    print(\"correct answer : \", to_sentence(test_pairs[randind][1],rev_shared_vocab))\n",
    "    print(\"prediction :\", to_sentence(decode(input_tensor=test_pairs[randind][0],encoder=our_encoder,decoder=our_decoder),rev_shared_vocab))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to evaluate our model by computing an evaluation score.\n",
    "We just compute the percentage of correct predictions over the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.982\n"
     ]
    }
   ],
   "source": [
    "score=0.\n",
    "for i in range(1000):\n",
    "    randind=np.random.randint(0,len(fr_test))\n",
    "    pred=to_sentence(decode(input_tensor=test_pairs[randind][0],encoder=our_encoder,decoder=our_decoder),rev_shared_vocab)\n",
    "    if int(num_test[randind])==int(pred):\n",
    "        score+=1\n",
    "print(score/1000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We see that with we provided a model that is able to solve this translation problem for round 98% of the cases. As a comparison, we wanted to beat the score of a simple encoder-decoder architecture that was able to solve this problem in 44% of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
