Last login: Sun Mar 17 21:24:07 on ttys000
Kenzas-MacBook-Pro:~ kenzaamara$ cd Desktop
Kenzas-MacBook-Pro:Desktop kenzaamara$ cd RNN_learning_to_execute/
Kenzas-MacBook-Pro:RNN_learning_to_execute kenzaamara$ git pull
Updating c2c81fa..58eb229
Fast-forward
 data.zip                                           |    Bin 0 -> 2882130 bytes
 data/eng-fra.txt                                   | 135842 ++++++++++++++++++
 data/names/Arabic.txt                              |   2000 +
 data/names/Chinese.txt                             |    268 +
 data/names/Czech.txt                               |    519 +
 data/names/Dutch.txt                               |    297 +
 data/names/English.txt                             |   3668 +
 data/names/French.txt                              |    277 +
 data/names/German.txt                              |    724 +
 data/names/Greek.txt                               |    203 +
 data/names/Irish.txt                               |    232 +
 data/names/Italian.txt                             |    709 +
 data/names/Japanese.txt                            |    991 +
 data/names/Korean.txt                              |     94 +
 data/names/Polish.txt                              |    139 +
 data/names/Portuguese.txt                          |     74 +
 data/names/Russian.txt                             |   9408 ++
 data/names/Scottish.txt                            |    100 +
 data/names/Spanish.txt                             |    298 +
 data/names/Vietnamese.txt                          |     73 +
 seq2seq_translation_tutorial.ipynb                 |    748 +-
 .../__pycache__/tokenization.cpython-36.pyc        |    Bin 11524 -> 0 bytes
 .../__pycache__/tokenization.cpython-37.pyc        |    Bin 11505 -> 11493 bytes
 translation/first_deco.pt                          |    Bin 0 -> 2248586 bytes
 translation/first_enco.pt                          |    Bin 0 -> 1634988 bytes
 translation/main.py                                |     53 +
 translation/numberseqseq.py                        |    328 +
 translation/numberseqseq.pyc                       |    Bin 0 -> 8745 bytes
 translation/sanstitre0.py                          |      7 +-
 translation/saved_models/.DS_Store                 |    Bin 0 -> 6148 bytes
 translation/saved_models/first_deco.pt             |    Bin 0 -> 2248586 bytes
 translation/saved_models/first_enco.pt             |    Bin 0 -> 1634988 bytes
 translation/seq2seqattn.py                         |    863 +
 translation/test.py                                |    249 +
 translation/tokenization.py                        |     16 +-
 translation/tokenization.pyc                       |    Bin 13678 -> 13980 bytes
 36 files changed, 158142 insertions(+), 38 deletions(-)
 create mode 100644 data.zip
 create mode 100644 data/eng-fra.txt
 create mode 100644 data/names/Arabic.txt
 create mode 100644 data/names/Chinese.txt
 create mode 100644 data/names/Czech.txt
 create mode 100644 data/names/Dutch.txt
 create mode 100644 data/names/English.txt
 create mode 100644 data/names/French.txt
 create mode 100644 data/names/German.txt
 create mode 100644 data/names/Greek.txt
 create mode 100644 data/names/Irish.txt
 create mode 100644 data/names/Italian.txt
 create mode 100644 data/names/Japanese.txt
 create mode 100644 data/names/Korean.txt
 create mode 100644 data/names/Polish.txt
 create mode 100644 data/names/Portuguese.txt
 create mode 100644 data/names/Russian.txt
 create mode 100644 data/names/Scottish.txt
 create mode 100644 data/names/Spanish.txt
 create mode 100644 data/names/Vietnamese.txt
 delete mode 100644 translation/__pycache__/tokenization.cpython-36.pyc
 create mode 100644 translation/first_deco.pt
 create mode 100644 translation/first_enco.pt
 create mode 100644 translation/numberseqseq.py
 create mode 100644 translation/numberseqseq.pyc
 create mode 100644 translation/saved_models/.DS_Store
 create mode 100644 translation/saved_models/first_deco.pt
 create mode 100644 translation/saved_models/first_enco.pt
 create mode 100644 translation/seq2seqattn.py
 create mode 100644 translation/test.py
Kenzas-MacBook-Pro:RNN_learning_to_execute kenzaamara$ cd translation/
Kenzas-MacBook-Pro:translation kenzaamara$ ls
LSTM_mask		data_npy		sanstitre0.py
LSTM_mask.py		first_deco.pt		saved_models
LSTM_mask_trained.pt	first_enco.pt		seq2seqattn.py
README.md		main.py			test.py
__pycache__		numberseqseq.py		tokenization.py
class_Encoding.py	numberseqseq.pyc	tokenization.pyc
corrections_keras	plot.py			utils
data_creation.py	result
Kenzas-MacBook-Pro:translation kenzaamara$ ssh kenza@168.61.101.175
The authenticity of host '168.61.101.175 (168.61.101.175)' can't be established.
ECDSA key fingerprint is SHA256:L0uDfw3zAda8zpTH990l3QdOdvChdEI4ZNTPesRfYxM.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '168.61.101.175' (ECDSA) to the list of known hosts.
kenza@168.61.101.175's password: 
Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.18.0-1013-azure x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Sun Mar 17 20:43:56 UTC 2019

  System load:  0.0                Processes:           153
  Usage of /:   22.8% of 28.90GB   Users logged in:     0
  Memory usage: 1%                 IP address for eth0: 10.0.0.4
  Swap usage:   0%


  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

22 packages can be updated.
0 updates are security updates.


Last login: Fri Mar 15 17:26:05 2019 from 129.104.65.2
(base) kenza@RNN:~$ cd RNN_learning_to_execute/
(base) kenza@RNN:~/RNN_learning_to_execute$ cd translation/
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ python numberseqseq.py 
(60000, 20)
avant def
apres def
numberseqseq.py:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_tensor = torch.tensor(training_pair[0])
numberseqseq.py:223: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  target_tensor = torch.tensor(training_pair[1])
epoch :0 iter : 100
0.8598459210395815
epoch :0 iter : 200
0.6985751643180843
epoch :0 iter : 300
0.6860450072288515
epoch :0 iter : 400
0.666506688594818
epoch :0 iter : 500
0.6570919651985168
epoch :0 iter : 600
0.6617728714942933
epoch :0 iter : 700
0.6562626323699955
epoch :0 iter : 800
0.6606697402000428
epoch :0 iter : 900
0.6495925846099854
epoch :0 iter : 1000
0.668844554424286
epoch :0 iter : 1100
0.6789469380378723
epoch :0 iter : 1200
0.6585495848655698
epoch :0 iter : 1300
0.6526036405563351
epoch :0 iter : 1400
0.6451071448326111
epoch :0 iter : 1500
0.6274832801818848
epoch :0 iter : 1600
0.6250457286834719
epoch :0 iter : 1700
0.5934172577857972
epoch :0 iter : 1800
0.5814839816093447
epoch :0 iter : 1900
0.548009970664978
epoch :0 iter : 2000
0.531809266805649
epoch :0 iter : 2100
0.49478072404861456
epoch :0 iter : 2200
0.48927434563636774
epoch :0 iter : 2300
0.4743510518074034
epoch :0 iter : 2400
0.4294007046222684
epoch :0 iter : 2500
0.4389833366870878
epoch :0 iter : 2600
0.4052327449321746
epoch :0 iter : 2700
0.3967357664108277
epoch :0 iter : 2800
0.3938400161266328
epoch :0 iter : 2900
0.3973315160274503
epoch :0 iter : 3000
0.3744242203235626
epoch :0 iter : 3100
0.3601928241252898
epoch :0 iter : 3200
0.35648171687126157
epoch :0 iter : 3300
0.3047319669723511
epoch :0 iter : 3400
0.30039610028266905
epoch :0 iter : 3500
0.3153463563919068
epoch :0 iter : 3600
0.3245566172599792
epoch :0 iter : 3700
0.2771665525436402
epoch :0 iter : 3800
0.3342398159503936
epoch :0 iter : 3900
0.2908959956169128
epoch :0 iter : 4000
0.2933559646606445
epoch :0 iter : 4100
0.2666259415149688
epoch :0 iter : 4200
0.28827064871788016
epoch :0 iter : 4300
0.26494725227355953
epoch :0 iter : 4400
0.26209088587760926
epoch :0 iter : 4500
0.3078999009132385
epoch :0 iter : 4600
0.26753764390945434
epoch :0 iter : 4700
0.25212056255340587
epoch :0 iter : 4800
0.23075747132301325
epoch :0 iter : 4900
0.211046000957489
epoch :0 iter : 5000
0.22721675109863282
epoch :0 iter : 5100
0.2412790493965149
epoch :0 iter : 5200
0.21079769396781917
epoch :0 iter : 5300
0.21476190686225893
epoch :0 iter : 5400
0.20339973044395446
epoch :0 iter : 5500
0.22651135897636412
epoch :0 iter : 5600
0.23311174726486208
epoch :0 iter : 5700
0.20110003328323367
epoch :0 iter : 5800
0.21387607955932617
epoch :0 iter : 5900
0.2137247567176819
epoch :0 iter : 6000
0.1994936473369598
epoch :0 iter : 6100
0.1735211715698243
epoch :0 iter : 6200
0.2193322536945342
epoch :0 iter : 6300
0.19085561466217038
epoch :0 iter : 6400
0.20519627451896677
epoch :0 iter : 6500
0.18072210025787355
epoch :0 iter : 6600
0.2029884419441223
epoch :0 iter : 6700
0.21789609909057628
epoch :0 iter : 6800
0.18521816992759704
epoch :0 iter : 6900
0.15993856596946723
epoch :0 iter : 7000
0.14422987961769104
epoch :0 iter : 7100
0.15763702702522278
epoch :0 iter : 7200
0.14988542318344117
epoch :0 iter : 7300
0.1506166305541992
epoch :0 iter : 7400
0.18527545452117916
epoch :0 iter : 7500
0.14317445588111874
epoch :0 iter : 7600
0.18618497729301456
epoch :0 iter : 7700
0.23547857141494752
epoch :0 iter : 7800
0.19979203629493722
epoch :0 iter : 7900
0.1270889711380005
epoch :0 iter : 8000
0.16581408953666676
epoch :0 iter : 8100
0.13786478972435
epoch :0 iter : 8200
0.10079963207244873
epoch :0 iter : 8300
0.13182915210723872
epoch :0 iter : 8400
0.1265499300956726
epoch :0 iter : 8500
0.14877264332771298
epoch :0 iter : 8600
0.13228366088867183
epoch :0 iter : 8700
0.14380309510231024
epoch :0 iter : 8800
0.14949541139602662
epoch :0 iter : 8900
0.08366377496719361
epoch :0 iter : 9000
0.09466889953613279
epoch :0 iter : 9100
0.08409649777412419
epoch :0 iter : 9200
0.08765277767181395
epoch :0 iter : 9300
0.11374500560760495
epoch :0 iter : 9400
0.10940827727317806
epoch :0 iter : 9500
0.10227959156036376
epoch :0 iter : 9600
0.07512623739242552
epoch :0 iter : 9700
0.06259876799583433
epoch :0 iter : 9800
0.06849045586585999
epoch :0 iter : 9900
0.05696807622909546
epoch :0 iter : 10000
0.0593692750930786
epoch :0 iter : 10100
0.04590332674980165
epoch :0 iter : 10200
0.05071890950202942
epoch :0 iter : 10300
0.06755820751190185
epoch :0 iter : 10400
0.08722365736961367
epoch :0 iter : 10500
0.07554149389266965
epoch :0 iter : 10600
0.05547218990325928
epoch :0 iter : 10700
0.04840767025947566
epoch :0 iter : 10800
0.046978622674942024
epoch :0 iter : 10900
0.04810753059387209
epoch :0 iter : 11000
0.10894968080520635
epoch :0 iter : 11100
0.088226104259491
epoch :0 iter : 11200
0.12055958032608036
epoch :0 iter : 11300
0.06365590953826902
epoch :0 iter : 11400
0.052839033603668206
epoch :0 iter : 11500
0.058162788867950434
epoch :0 iter : 11600
0.07162663817405704
epoch :0 iter : 11700
0.08694868755340579
epoch :0 iter : 11800
0.05394418716430665
epoch :0 iter : 11900
0.03713266038894653
epoch :0 iter : 12000
0.04401659107208253
epoch :0 iter : 12100
0.04055188655853271
epoch :0 iter : 12200
0.06991667819023131
epoch :0 iter : 12300
0.07422568511962892
epoch :0 iter : 12400
0.04518708539009097
epoch :0 iter : 12500
0.041670663356781
epoch :0 iter : 12600
0.08957376503944398
epoch :0 iter : 12700
0.05856717729568481
epoch :0 iter : 12800
0.06080063819885256
epoch :0 iter : 12900
0.08635062384605405
epoch :0 iter : 13000
0.10361717343330384
epoch :0 iter : 13100
0.10321476554870607
epoch :0 iter : 13200
0.05345193171501159
epoch :0 iter : 13300
0.07671689510345459
epoch :0 iter : 13400
0.07607384729385379
epoch :0 iter : 13500
0.10007234001159666
epoch :0 iter : 13600
0.1280446419715881
epoch :0 iter : 13700
0.08133461833000183
epoch :0 iter : 13800
0.07498151516914368
epoch :0 iter : 13900
0.10941704785823825
epoch :0 iter : 14000
0.10361452949047094
epoch :0 iter : 14100
0.08454579472541807
epoch :0 iter : 14200
0.05530918335914611
epoch :0 iter : 14300
0.03619103217124941
epoch :0 iter : 14400
0.05954785275459287
epoch :0 iter : 14500
0.045980215787887586
epoch :0 iter : 14600
0.06706691026687622
epoch :0 iter : 14700
0.10921148180961607
epoch :0 iter : 14800
0.08706344962120056
epoch :0 iter : 14900
0.04207973051071167
epoch :0 iter : 15000
0.05308406066894531
epoch :0 iter : 15100
0.034951062440872205
epoch :0 iter : 15200
0.04142010855674745
epoch :0 iter : 15300
0.024831549882888798
epoch :0 iter : 15400
0.04295742988586425
epoch :0 iter : 15500
0.04341879320144652
epoch :0 iter : 15600
0.0345768530368805
epoch :0 iter : 15700
0.04269790554046629
epoch :0 iter : 15800
0.03393062281608582
epoch :0 iter : 15900
0.05402224612236025
epoch :0 iter : 16000
0.029957054376602173
epoch :0 iter : 16100
0.03474345993995667
epoch :0 iter : 16200
0.022940328836441037
epoch :0 iter : 16300
0.02609158730506897
epoch :0 iter : 16400
0.03380749511718751
epoch :0 iter : 16500
0.02457333827018739
epoch :0 iter : 16600
0.03292950415611267
epoch :0 iter : 16700
0.048171567201614404
epoch :0 iter : 16800
0.038244602203369144
epoch :0 iter : 16900
0.037344663143157965
epoch :0 iter : 17000
0.030032065391540522
epoch :0 iter : 17100
0.026244732618331906
epoch :0 iter : 17200
0.020566489696502686
epoch :0 iter : 17300
0.01842340159416199
epoch :0 iter : 17400
0.025212697744369503
epoch :0 iter : 17500
0.017611394405364994
epoch :0 iter : 17600
0.05566031932830809
epoch :0 iter : 17700
0.03024680852890014
epoch :0 iter : 17800
0.04311099958419797
epoch :0 iter : 17900
0.02490529608726501
epoch :0 iter : 18000
0.044476915836334216
epoch :0 iter : 18100
0.04993435239791871
epoch :0 iter : 18200
0.03932519197463989
epoch :0 iter : 18300
0.054951321125030504
epoch :0 iter : 18400
0.07501312828063965
epoch :0 iter : 18500
0.05356015157699587
epoch :0 iter : 18600
0.03159575247764588
epoch :0 iter : 18700
0.052670693397521964
epoch :0 iter : 18800
0.025963244676589968
epoch :0 iter : 18900
0.03070523929595947
epoch :0 iter : 19000
0.04490990746021271
epoch :0 iter : 19100
0.04650944566726684
epoch :0 iter : 19200
0.02584899353981018
epoch :0 iter : 19300
0.02339951944351197
epoch :0 iter : 19400
0.033957522869110114
epoch :0 iter : 19500
0.021609202861785887
epoch :0 iter : 19600
0.024363147258758548
epoch :0 iter : 19700
0.025506517410278314
epoch :0 iter : 19800
0.01151929879188538
epoch :0 iter : 19900
0.009535399675369262
epoch :0 iter : 20000
0.023663397789001464
epoch :1 iter : 100
0.020486919164657585
epoch :1 iter : 200
0.016965159416198732
epoch :1 iter : 300
0.028176473140716553
epoch :1 iter : 400
0.031206601142883306
epoch :1 iter : 500
0.047398218393325815
epoch :1 iter : 600
0.025136057376861552
epoch :1 iter : 700
0.031027729988098156
epoch :1 iter : 800
0.00930822610855103
epoch :1 iter : 900
0.02041607117652894
epoch :1 iter : 1000
0.031219780445098867
epoch :1 iter : 1100
0.01646515631675721
epoch :1 iter : 1200
0.01904378008842468
epoch :1 iter : 1300
0.010302847862243652
epoch :1 iter : 1400
0.009091002225875854
epoch :1 iter : 1500
0.012702617645263668
epoch :1 iter : 1600
0.02349600458145143
epoch :1 iter : 1700
0.04072766590118408
epoch :1 iter : 1800
0.011913784980773928
epoch :1 iter : 1900
0.015548406839370737
epoch :1 iter : 2000
0.01224158668518066
epoch :1 iter : 2100
0.01975976705551147
epoch :1 iter : 2200
0.013674882888793944
epoch :1 iter : 2300
0.01752882754802704
epoch :1 iter : 2400
0.017504159927368174
epoch :1 iter : 2500
0.013547705650329596
epoch :1 iter : 2600
0.013352223396301268
epoch :1 iter : 2700
0.01929905152320862
epoch :1 iter : 2800
0.01610491752624512
epoch :1 iter : 2900
0.013067931652069088
epoch :1 iter : 3000
0.013643941402435291
epoch :1 iter : 3100
0.017770711421966554
epoch :1 iter : 3200
0.026072524070739737
epoch :1 iter : 3300
0.012429015636444085
epoch :1 iter : 3400
0.021228709220886235
epoch :1 iter : 3500
0.013383134603500357
epoch :1 iter : 3600
0.02103730297088623
epoch :1 iter : 3700
0.008365746974945066
epoch :1 iter : 3800
0.014453433752059939
epoch :1 iter : 3900
0.020310616731643666
epoch :1 iter : 4000
0.00969562888145447
epoch :1 iter : 4100
0.016450799703598022
epoch :1 iter : 4200
0.02160936260223388
epoch :1 iter : 4300
0.023133576869964596
epoch :1 iter : 4400
0.009423679590225222
epoch :1 iter : 4500
0.015078334331512446
epoch :1 iter : 4600
0.005570169448852537
epoch :1 iter : 4700
0.020601819038391117
epoch :1 iter : 4800
0.014623461723327638
epoch :1 iter : 4900
0.024341444253921516
epoch :1 iter : 5000
0.011076944112777707
epoch :1 iter : 5100
0.013607839822769164
epoch :1 iter : 5200
0.019795717477798454
epoch :1 iter : 5300
0.015581443071365357
epoch :1 iter : 5400
0.026877624750137333
epoch :1 iter : 5500
0.03169665622711181
epoch :1 iter : 5600
0.02031663084030151
epoch :1 iter : 5700
0.029297369003295902
epoch :1 iter : 5800
0.010676944732666015
epoch :1 iter : 5900
0.017030025482177736
epoch :1 iter : 6000
0.01824173188209534
epoch :1 iter : 6100
0.013609706163406373
epoch :1 iter : 6200
0.02877771115303039
epoch :1 iter : 6300
0.007042286157608034
epoch :1 iter : 6400
0.015117563962936396
epoch :1 iter : 6500
0.01569608473777771
epoch :1 iter : 6600
0.02697183442115783
epoch :1 iter : 6700
0.016998464345932007
epoch :1 iter : 6800
0.017130301713943488
epoch :1 iter : 6900
0.014440523862838752
epoch :1 iter : 7000
0.00870255994796753
epoch :1 iter : 7100
0.017356117010116565
epoch :1 iter : 7200
0.004355780601501464
epoch :1 iter : 7300
0.022127955913543693
epoch :1 iter : 7400
0.017878051757812503
epoch :1 iter : 7500
0.009945991277694702
epoch :1 iter : 7600
0.013291005611419682
epoch :1 iter : 7700
0.030422313690185544
epoch :1 iter : 7800
0.018754131793975833
epoch :1 iter : 7900
0.01233507990837097
epoch :1 iter : 8000
0.02326283264160156
epoch :1 iter : 8100
0.011286633968353272
epoch :1 iter : 8200
0.016326856136322017
epoch :1 iter : 8300
0.013962836027145386
epoch :1 iter : 8400
0.01407532644271851
epoch :1 iter : 8500
0.014415518999099725
epoch :1 iter : 8600
0.010994302034378053
epoch :1 iter : 8700
0.007042917966842652
epoch :1 iter : 8800
0.007439506053924561
epoch :1 iter : 8900
0.008754428863525393
epoch :1 iter : 9000
0.010470324754714967
epoch :1 iter : 9100
0.008838847637176517
epoch :1 iter : 9200
0.008010825395584106
epoch :1 iter : 9300
0.012815861225128171
epoch :1 iter : 9400
0.014442346334457401
epoch :1 iter : 9500
0.022235292196273803
epoch :1 iter : 9600
0.007678273916244506
epoch :1 iter : 9700
0.005895756006240847
epoch :1 iter : 9800
0.0048421788215637215
epoch :1 iter : 9900
0.005027565479278565
epoch :1 iter : 10000
0.005085534095764164
epoch :1 iter : 10100
0.0035857486724853514
epoch :1 iter : 10200
0.005912906169891357
epoch :1 iter : 10300
0.01059671330451965
epoch :1 iter : 10400
0.016089329004287722
epoch :1 iter : 10500
0.01616510009765626
epoch :1 iter : 10600
0.006402418613433838
epoch :1 iter : 10700
0.022521342039108275
epoch :1 iter : 10800
0.0057797887325286845
epoch :1 iter : 10900
0.01360653591156006
epoch :1 iter : 11000
0.011212682962417599
epoch :1 iter : 11100
0.013952064514160156
epoch :1 iter : 11200
0.008225591659545901
epoch :1 iter : 11300
0.007978215217590331
epoch :1 iter : 11400
0.005742521286010742
epoch :1 iter : 11500
0.010415702342987054
epoch :1 iter : 11600
0.005945228576660157
epoch :1 iter : 11700
0.007878140211105346
epoch :1 iter : 11800
0.010891130208969122
epoch :1 iter : 11900
0.005363748550415041
epoch :1 iter : 12000
0.008936190366744996
epoch :1 iter : 12100
0.01735720443725586
epoch :1 iter : 12200
0.015367084026336665
epoch :1 iter : 12300
0.02346236991882324
epoch :1 iter : 12400
0.01137403130531311
epoch :1 iter : 12500
0.010373614549636841
epoch :1 iter : 12600
0.0035987625122070317
epoch :1 iter : 12700
0.0056992149353027365
epoch :1 iter : 12800
0.007640860080718997
epoch :1 iter : 12900
0.01053182482719422
epoch :1 iter : 13000
0.00571054220199585
epoch :1 iter : 13100
0.003522165536880494
epoch :1 iter : 13200
0.0026284387111663826
epoch :1 iter : 13300
0.01605982184410095
epoch :1 iter : 13400
0.018683712959289545
epoch :1 iter : 13500
0.009903090953826903
epoch :1 iter : 13600
0.012908843040466314
epoch :1 iter : 13700
0.00382796335220337
epoch :1 iter : 13800
0.007903703689575195
epoch :1 iter : 13900
0.009652845382690433
epoch :1 iter : 14000
0.007861116170883179
epoch :1 iter : 14100
0.006881920814514161
epoch :1 iter : 14200
0.005972602605819701
epoch :1 iter : 14300
0.004728452920913697
epoch :1 iter : 14400
0.0035809540748596197
epoch :1 iter : 14500
0.0018107738494873046
epoch :1 iter : 14600
0.009894788026809693
epoch :1 iter : 14700
0.00652948260307312
epoch :1 iter : 14800
0.00968368887901306
epoch :1 iter : 14900
0.00920407724380493
epoch :1 iter : 15000
0.004983253955841065
epoch :1 iter : 15100
0.004050543546676637
epoch :1 iter : 15200
0.007670633554458621
epoch :1 iter : 15300
0.0055923912525177024
epoch :1 iter : 15400
0.007766724109649657
epoch :1 iter : 15500
0.004489771604537965
epoch :1 iter : 15600
0.009852548122406005
epoch :1 iter : 15700
0.02034222817420959
epoch :1 iter : 15800
0.012182501792907717
epoch :1 iter : 15900
0.009470539808273315
epoch :1 iter : 16000
0.012607217311859131
epoch :1 iter : 16100
0.013412371158599855
epoch :1 iter : 16200
0.010781241416931153
epoch :1 iter : 16300
0.0039020352363586435
epoch :1 iter : 16400
0.009340435743331911
epoch :1 iter : 16500
0.006089307308197023
epoch :1 iter : 16600
0.00719668698310852
epoch :1 iter : 16700
0.004703776359558106
epoch :1 iter : 16800
0.006940375804901121
epoch :1 iter : 16900
0.004761086225509643
epoch :1 iter : 17000
0.010936979293823243
epoch :1 iter : 17100
0.00893807625770569
epoch :1 iter : 17200
0.012560449838638313
epoch :1 iter : 17300
0.003148158073425294
epoch :1 iter : 17400
0.005857984066009519
epoch :1 iter : 17500
0.0057778418064117435
epoch :1 iter : 17600
0.007807753086090092
epoch :1 iter : 17700
0.006981990814208983
epoch :1 iter : 17800
0.00717137765884399
epoch :1 iter : 17900
0.005355513811111447
epoch :1 iter : 18000
0.01607082629203797
epoch :1 iter : 18100
0.005110265731811522
epoch :1 iter : 18200
0.004571065425872803
epoch :1 iter : 18300
0.011861623764038091
epoch :1 iter : 18400
0.02216378688812256
epoch :1 iter : 18500
0.023038478136062638
epoch :1 iter : 18600
0.003614047050476075
epoch :1 iter : 18700
0.014111981868743892
epoch :1 iter : 18800
0.01902635240554809
epoch :1 iter : 18900
0.006235512733459473
epoch :1 iter : 19000
0.010526129722595218
epoch :1 iter : 19100
0.024522715091705324
epoch :1 iter : 19200
0.008246706485748293
epoch :1 iter : 19300
0.003888885021209718
epoch :1 iter : 19400
0.0056590456962585444
epoch :1 iter : 19500
0.003442487955093384
epoch :1 iter : 19600
0.006408401966094971
epoch :1 iter : 19700
0.012248945713043215
epoch :1 iter : 19800
0.0037691068649292002
epoch :1 iter : 19900
0.0021052293777465815
epoch :1 iter : 20000
0.007242851257324224
epoch :2 iter : 100
0.002829859972000121
epoch :2 iter : 200
0.0054986455440521245
epoch :2 iter : 300
0.008821205139160156
epoch :2 iter : 400
0.006763065814971924
epoch :2 iter : 500
0.012467381238937378
epoch :2 iter : 600
0.009300854444503787
epoch :2 iter : 700
0.010467614173889162
epoch :2 iter : 800
0.0030437011718750006
epoch :2 iter : 900
0.004567980289459229
epoch :2 iter : 1000
0.009549546718597412
epoch :2 iter : 1100
0.004221818685531616
epoch :2 iter : 1200
0.005744947195053102
epoch :2 iter : 1300
0.00237730598449707
epoch :2 iter : 1400
0.0010289802551269533
epoch :2 iter : 1500
0.005253077983856202
epoch :2 iter : 1600
0.006774577856063842
epoch :2 iter : 1700
0.006638647556304929
epoch :2 iter : 1800
0.0035983877182006827
epoch :2 iter : 1900
0.007038569211959841
epoch :2 iter : 2000
0.0024974460601806634
epoch :2 iter : 2100
0.00717515516281128
epoch :2 iter : 2200
0.0032649085521697994
epoch :2 iter : 2300
0.00637638759613037
epoch :2 iter : 2400
0.004155100345611573
epoch :2 iter : 2500
0.002796700000762939
epoch :2 iter : 2600
0.005812714815139771
epoch :2 iter : 2700
0.008355027914047243
epoch :2 iter : 2800
0.006382403850555421
epoch :2 iter : 2900
0.004631130218505859
epoch :2 iter : 3000
0.005047479629516603
epoch :2 iter : 3100
0.013912162780761718
epoch :2 iter : 3200
0.007312563657760622
epoch :2 iter : 3300
0.003961714029312133
epoch :2 iter : 3400
0.004397058963775635
epoch :2 iter : 3500
0.004499732494354248
epoch :2 iter : 3600
0.007478165864944457
epoch :2 iter : 3700
0.002464759349822998
epoch :2 iter : 3800
0.004599566936492921
epoch :2 iter : 3900
0.008364796161651611
epoch :2 iter : 4000
0.0037286324501037598
epoch :2 iter : 4100
0.00784432601928711
epoch :2 iter : 4200
0.00555808615684509
epoch :2 iter : 4300
0.006554192543029786
epoch :2 iter : 4400
0.0034794697761535635
epoch :2 iter : 4500
0.0021327872276306154
epoch :2 iter : 4600
0.001478825569152832
epoch :2 iter : 4700
0.0065220606327056905
epoch :2 iter : 4800
0.004528804779052735
epoch :2 iter : 4900
0.0023076682090759277
epoch :2 iter : 5000
0.002756268978118896
epoch :2 iter : 5100
0.004738479852676392
epoch :2 iter : 5200
0.002411904573440552
epoch :2 iter : 5300
0.004514926910400391
epoch :2 iter : 5400
0.007475330591201784
epoch :2 iter : 5500
0.016282928466796874
epoch :2 iter : 5600
0.006443061828613281
epoch :2 iter : 5700
0.12408438730239864
epoch :2 iter : 5800
0.038874470233917234
epoch :2 iter : 5900
0.043744668722152706
epoch :2 iter : 6000
0.024698883771896343
epoch :2 iter : 6100
0.00748064041137695
epoch :2 iter : 6200
0.021316204786300662
epoch :2 iter : 6300
0.022891729354858406
epoch :2 iter : 6400
0.016419094562530517
epoch :2 iter : 6500
0.016575783252716057
epoch :2 iter : 6600
0.03525015115737915
epoch :2 iter : 6700
0.03572290349006653
epoch :2 iter : 6800
0.022259243130683896
epoch :2 iter : 6900
0.024390408992767332
epoch :2 iter : 7000
0.016560909509658817
epoch :2 iter : 7100
0.01841928887367248
epoch :2 iter : 7200
0.01080908727645874
epoch :2 iter : 7300
0.020923944950103763
epoch :2 iter : 7400
0.014565618515014646
epoch :2 iter : 7500
0.019145143747329715
epoch :2 iter : 7600
0.038018821239471444
epoch :2 iter : 7700
0.020583896160125723
epoch :2 iter : 7800
0.00895854377746582
epoch :2 iter : 7900
0.012655637741088863
epoch :2 iter : 8000
0.03822642183303834
epoch :2 iter : 8100
0.014456796646118167
epoch :2 iter : 8200
0.011077053070068366
epoch :2 iter : 8300
0.009216462612152101
epoch :2 iter : 8400
0.008303466796874999
epoch :2 iter : 8500
0.011846765756607059
epoch :2 iter : 8600
0.0134215178489685
epoch :2 iter : 8700
0.012104692459106446
epoch :2 iter : 8800
0.0065852680206298815
epoch :2 iter : 8900
0.007861049890518187
epoch :2 iter : 9000
0.017329828262329098
epoch :2 iter : 9100
0.012934388399124148
epoch :2 iter : 9200
0.005691384792327881
epoch :2 iter : 9300
0.008671694755554201
epoch :2 iter : 9400
0.010009049892425535
epoch :2 iter : 9500
0.02149448013305665
epoch :2 iter : 9600
0.006202265024185182
epoch :2 iter : 9700
0.00277556610107422
epoch :2 iter : 9800
0.005314441204071044
epoch :2 iter : 9900
0.007326028108596802
epoch :2 iter : 10000
0.007646010875701908
epoch :2 iter : 10100
0.005539812803268433
epoch :2 iter : 10200
0.003623172283172607
epoch :2 iter : 10300
0.004914967298507693
epoch :2 iter : 10400
0.01031219005584717
epoch :2 iter : 10500
0.0061212573051452656
epoch :2 iter : 10600
0.004684192657470701
epoch :2 iter : 10700
0.0063812651634216335
epoch :2 iter : 10800
0.002154357433319091
epoch :2 iter : 10900
0.012063146591186527
epoch :2 iter : 11000
0.008976901531219477
epoch :2 iter : 11100
0.008748034000396728
epoch :2 iter : 11200
0.002358069419860839
epoch :2 iter : 11300
0.003962944507598876
epoch :2 iter : 11400
0.0030257558822631837
epoch :2 iter : 11500
0.004572441577911377
epoch :2 iter : 11600
0.001643204212188721
epoch :2 iter : 11700
0.0016316428184509275
epoch :2 iter : 11800
0.004784151077270508
epoch :2 iter : 11900
0.003787617683410644
epoch :2 iter : 12000
0.004054925441741943
epoch :2 iter : 12100
0.005411889076232912
epoch :2 iter : 12200
0.009991719007492074
epoch :2 iter : 12300
0.02073348236083985
epoch :2 iter : 12400
0.0018715882301330576
epoch :2 iter : 12500
0.004289021968841552
epoch :2 iter : 12600
0.001670724868774413
epoch :2 iter : 12700
0.008320055961608888
epoch :2 iter : 12800
0.005155084371566774
epoch :2 iter : 12900
0.005984148740768433
epoch :2 iter : 13000
0.004095828533172607
epoch :2 iter : 13100
0.0024609432220458983
epoch :2 iter : 13200
0.003790406703948975
epoch :2 iter : 13300
0.010303442955017087
epoch :2 iter : 13400
0.007442706584930422
epoch :2 iter : 13500
0.005898483753204345
epoch :2 iter : 13600
0.009311149358749389
epoch :2 iter : 13700
0.010460452556610102
epoch :2 iter : 13800
0.006934749841690064
epoch :2 iter : 13900
0.004500555515289308
epoch :2 iter : 14000
0.004152642011642457
epoch :2 iter : 14100
0.010680308818817137
epoch :2 iter : 14200
0.004711810827255248
epoch :2 iter : 14300
0.0039101581573486345
epoch :2 iter : 14400
0.0027286279201507563
epoch :2 iter : 14500
0.0016043968200683593
epoch :2 iter : 14600
0.009592447996139529
epoch :2 iter : 14700
0.006826721429824829
epoch :2 iter : 14800
0.00637605929374695
epoch :2 iter : 14900
0.006784605264663698
epoch :2 iter : 15000
0.009133016109466555
epoch :2 iter : 15100
0.008270174264907842
epoch :2 iter : 15200
0.009150073289871214
epoch :2 iter : 15300
0.005433672904968264
epoch :2 iter : 15400
0.002628849029541015
epoch :2 iter : 15500
0.00481498670578003
epoch :2 iter : 15600
0.004396717309951782
epoch :2 iter : 15700
0.01186918067932129
epoch :2 iter : 15800
0.0036832854747772216
epoch :2 iter : 15900
0.003789288520812986
epoch :2 iter : 16000
0.0009803826808929448
epoch :2 iter : 16100
0.007361212730407713
epoch :2 iter : 16200
0.0037955377101898196
epoch :2 iter : 16300
0.0014789409637451167
epoch :2 iter : 16400
0.002346912860870362
epoch :2 iter : 16500
0.0011191644668579102
epoch :2 iter : 16600
0.003692403078079223
epoch :2 iter : 16700
0.0013445482254028323
epoch :2 iter : 16800
0.002580984592437744
epoch :2 iter : 16900
0.0019625060558319096
epoch :2 iter : 17000
0.0025219244956970217
epoch :2 iter : 17100
0.002514623880386353
epoch :2 iter : 17200
0.0004201993942260744
epoch :2 iter : 17300
0.0016080312728881832
epoch :2 iter : 17400
0.002290604591369629
epoch :2 iter : 17500
0.003940983772277832
epoch :2 iter : 17600
0.0037641532421112058
epoch :2 iter : 17700
0.001559613227844239
epoch :2 iter : 17800
0.0027345352172851558
epoch :2 iter : 17900
0.0024128670692443845
epoch :2 iter : 18000
0.00779593825340271
epoch :2 iter : 18100
0.002560322999954223
epoch :2 iter : 18200
0.025368870019912722
epoch :2 iter : 18300
0.018719769835472102
epoch :2 iter : 18400
0.02539103603363037
epoch :2 iter : 18500
0.011561434745788569
epoch :2 iter : 18600
0.0012471866607666015
epoch :2 iter : 18700
0.016282756805419928
epoch :2 iter : 18800
0.004093091249465943
epoch :2 iter : 18900
0.004389745712280274
epoch :2 iter : 19000
0.005776797294616701
epoch :2 iter : 19100
0.0049237229824066146
epoch :2 iter : 19200
0.0029323916435241705
epoch :2 iter : 19300
0.003044540882110595
epoch :2 iter : 19400
0.0034030051231384285
epoch :2 iter : 19500
0.001193148136138916
epoch :2 iter : 19600
0.003103026866912842
epoch :2 iter : 19700
0.007027745246887207
epoch :2 iter : 19800
0.0007916131019592288
epoch :2 iter : 19900
0.0008746795654296878
epoch :2 iter : 20000
0.0033633575439453124
numberseqseq.py:300: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_test=torch.tensor(pairs[i][0])
numberseqseq.py:301: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y_test=torch.tensor(pairs[i][1])
tensor(6)
tensor(4)
tensor(8)
tensor(13)
tensor(6)
tensor(6)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([ 6.,  4.,  8., 13.,  6.,  6.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[ 6],
        [ 4],
        [ 8],
        [13],
        [ 6],
        [ 6],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(12)
tensor(4)
tensor(11)
tensor(13)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([12.,  4., 11., 13.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[12],
        [ 4],
        [11],
        [13],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(5)
tensor(13)
tensor(6)
tensor(13)
tensor(8)
tensor(12)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([ 5., 13.,  6., 13.,  8., 12.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[ 5],
        [13],
        [ 6],
        [13],
        [ 8],
        [12],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(13)
tensor(8)
tensor(8)
tensor(11)
tensor(4)
tensor(11)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([13.,  8.,  8., 11.,  4., 11.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[13],
        [ 8],
        [ 8],
        [11],
        [ 4],
        [11],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(5)
tensor(7)
tensor(4)
tensor(12)
tensor(10)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([ 5.,  7.,  4., 12., 10.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[ 5],
        [ 7],
        [ 4],
        [12],
        [10],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(6)
tensor(10)
tensor(9)
tensor(4)
tensor(13)
tensor(11)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([ 6., 10.,  9.,  4., 13., 11.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[ 6],
        [10],
        [ 9],
        [ 4],
        [13],
        [11],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(12)
tensor(10)
tensor(12)
tensor(8)
tensor(6)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([12., 10., 12.,  8.,  6.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[12],
        [10],
        [12],
        [ 8],
        [ 6],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(9)
tensor(11)
tensor(13)
tensor(9)
tensor(10)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([ 9., 11., 13.,  9., 10.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[ 9],
        [11],
        [13],
        [ 9],
        [10],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(10)
tensor(11)
tensor(7)
tensor(7)
tensor(5)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([10., 11.,  7.,  7.,  5.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[10],
        [11],
        [ 7],
        [ 7],
        [ 5],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
tensor(12)
tensor(5)
tensor(11)
tensor(12)
tensor(6)
tensor(4)
tensor(2)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor(0)
tensor([12.,  5., 11., 12.,  6.,  4.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.])
tensor([[12],
        [ 5],
        [11],
        [12],
        [ 6],
        [ 4],
        [ 2],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0],
        [ 0]])
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git add *
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git commit -m "coucou c'est @Kenza Amara"
[master 53fb665] coucou c'est @Kenza Amara
 Committer: Ubuntu <kenza@RNN.bbt0000t4o1uhhdenqparbympe.fx.internal.cloudapp.net>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly. Run the
following command and follow the instructions in your editor to edit
your configuration file:

    git config --global --edit

After doing this, you may fix the identity used for this commit with:

    git commit --amend --reset-author

 2 files changed, 0 insertions(+), 0 deletions(-)
 rewrite translation/first_deco.pt (75%)
 rewrite translation/first_enco.pt (80%)
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git push
Username for 'https://github.com': k-amara
Password for 'https://k-amara@github.com': 
To https://github.com/barthelemymp/RNN_learning_to_execute.git
 ! [rejected]        master -> master (fetch first)
error: failed to push some refs to 'https://github.com/barthelemymp/RNN_learning_to_execute.git'
hint: Updates were rejected because the remote contains work that you do
hint: not have locally. This is usually caused by another repository pushing
hint: to the same ref. You may want to first integrate the remote changes
hint: (e.g., 'git pull ...') before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ Connection to 168.61.101.175 closed by remote host.
Connection to 168.61.101.175 closed.
Kenzas-MacBook-Pro:translation kenzaamara$ 
  [Restored 18 Mar 2019 at 08:14:36]
Last login: Mon Mar 18 08:14:36 on ttys000
Kenzas-MacBook-Pro:translation kenzaamara$ ssh kenza@40.113.4.8
The authenticity of host '40.113.4.8 (40.113.4.8)' can't be established.
ECDSA key fingerprint is SHA256:L0uDfw3zAda8zpTH990l3QdOdvChdEI4ZNTPesRfYxM.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '40.113.4.8' (ECDSA) to the list of known hosts.
kenza@40.113.4.8's password: 
Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.18.0-1013-azure x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Mon Mar 18 17:36:48 UTC 2019

  System load:  0.51               Processes:           163
  Usage of /:   22.8% of 28.90GB   Users logged in:     0
  Memory usage: 1%                 IP address for eth0: 10.0.0.4
  Swap usage:   0%

 * Ubuntu's Kubernetes 1.14 distributions can bypass Docker and use containerd
   directly, see https://bit.ly/ubuntu-containerd or try it now with

     snap install microk8s --channel=1.14/beta --classic

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

22 packages can be updated.
0 updates are security updates.


Last login: Mon Mar 18 07:13:45 2019 from 129.104.65.2
(base) kenza@RNN:~$ cd Desktop
-bash: cd: Desktop: No such file or directory
(base) kenza@RNN:~$ cd RNN_learning_to_execute/
(base) kenza@RNN:~/RNN_learning_to_execute$ cd translation/
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ ls
LSTM_mask          corrections_keras  main.py          seq2seqattn.py
LSTM_mask.py       data_creation.py   numberseqseq.py  test.py
README.md          data_npy           plot.py          tokenization.py
__pycache__        first_deco.pt      result           tokenization.pyc
class_Encoding.py  first_enco.pt      sanstitre0.py    utils
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git pull
remote: Enumerating objects: 23, done.
remote: Counting objects: 100% (23/23), done.
remote: Compressing objects: 100% (8/8), done.
remote: Total 17 (delta 12), reused 14 (delta 9), pack-reused 0
Unpacking objects: 100% (17/17), done.
From https://github.com/barthelemymp/RNN_learning_to_execute
   a224676..58eb229  master     -> origin/master
Merge made by the 'recursive' strategy.
 translation/main.py                    |  53 +++++++++++++++++++++++++++++++++
 translation/numberseqseq.py            |  43 +++++++++++---------------
 translation/numberseqseq.pyc           | Bin 0 -> 8745 bytes
 translation/saved_models/.DS_Store     | Bin 0 -> 6148 bytes
 translation/saved_models/first_deco.pt | Bin 0 -> 2248586 bytes
 translation/saved_models/first_enco.pt | Bin 0 -> 1634988 bytes
 translation/tokenization.py            |   8 +++++
 translation/tokenization.pyc           | Bin 13674 -> 13980 bytes
 8 files changed, 78 insertions(+), 26 deletions(-)
 create mode 100644 translation/numberseqseq.pyc
 create mode 100644 translation/saved_models/.DS_Store
 create mode 100644 translation/saved_models/first_deco.pt
 create mode 100644 translation/saved_models/first_enco.pt
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git pull
Already up to date.
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ vi numberseqseq.py
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ python LSTM_mask.py 
tensor([ 6, 11,  9,  7, 10, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  7,  9,  9, 13,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0, 12, 12, 10, 11,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0, 13, 11,  7, 13,  8,  6,  2,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0])
Traceback (most recent call last):
  File "LSTM_mask.py", line 235, in <module>
    loss = model.loss(batch_pred, batch_y)
  File "LSTM_mask.py", line 167, in loss
    Y_hat = Y_hat.view(-1, self.nb_tags)
RuntimeError: invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view(). at /opt/conda/conda-bld/pytorch_1549636813070/work/aten/src/TH/generic/THTensor.cpp:213
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ vi LSTM_mask
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git add *
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git commit -m "coucou c'est @Kenza Amara"
[master 8789dd4] coucou c'est @Kenza Amara
 Committer: Ubuntu <kenza@RNN.bbt0000t4o1uhhdenqparbympe.fx.internal.cloudapp.net>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly. Run the
following command and follow the instructions in your editor to edit
your configuration file:

    git config --global --edit

After doing this, you may fix the identity used for this commit with:

    git commit --amend --reset-author

 3 files changed, 0 insertions(+), 0 deletions(-)
 rewrite translation/first_deco.pt (75%)
 rewrite translation/first_enco.pt (80%)
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ git push
Username for 'https://github.com': k-amara
Password for 'https://k-amara@github.com': 
Counting objects: 15, done.
Delta compression using up to 6 threads.
Compressing objects: 100% (15/15), done.
Writing objects: 100% (15/15), 6.83 MiB | 6.15 MiB/s, done.
Total 15 (delta 7), reused 0 (delta 0)
remote: Resolving deltas: 100% (7/7), completed with 4 local objects.
To https://github.com/barthelemymp/RNN_learning_to_execute.git
   58eb229..8789dd4  master -> master
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ vi LSTM_mask
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ ls
LSTM_mask          data_creation.py  numberseqseq.pyc  test.py
LSTM_mask.py       data_npy          plot.py           tokenization.py
README.md          first_deco.pt     result            tokenization.pyc
__pycache__        first_enco.pt     sanstitre0.py     utils
class_Encoding.py  main.py           saved_models
corrections_keras  numberseqseq.py   seq2seqattn.py
(base) kenza@RNN:~/RNN_learning_to_execute/translation$ vi LSTM_mask.py

        # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence
        # and calculate the loss on that.

        # flatten all the labels
        Y= Y.contiguous()
        Y = Y.view(-1)
        print(Y)
        
        # flatten all predictions
        Y_hat = Y_hat.contiguous()
        Y_hat = Y_hat.view(-1, self.nb_tags)
        print(Y_hat)
        # create a mask by filtering out all tokens that ARE NOT the padding token
        tag_pad_token = self.tags['_PAD']
        mask = (Y > tag_pad_token).float()

        # count how many tokens we have
        nb_tokens = int(torch.sum(mask).data[0])

        # pick the values for the label and zero out the rest with the mask
        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask
-- INSERT --                                                  164,9         62%
